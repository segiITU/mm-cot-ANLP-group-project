{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "#from utils_data import ScienceQADatasetImg\n",
    "from src.original.model import T5ForMultimodalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset\n",
    "import evaluate\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map keys, add features, add captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = json.load(open(\"data/problems.json\"))\n",
    "name_maps = json.load(open(\"data/name_map.json\"))\n",
    "captions = json.load(open(\"data/instruct_captions.json\"))[\"captions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = np.load(\"vision_features/vision_features/clip.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21208"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "if problems[\"7\"][\"image\"]:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "problems_s = dict()\n",
    "for qid in problems:\n",
    "    idx+=1\n",
    "    problems_s[qid]=problems[qid]\n",
    "    problems_s[qid]['caption'] = captions[qid] if qid in captions else \"\"\n",
    "    if str(qid) in name_maps:\n",
    "        problems_s[qid]['image_feature'] = image_features[int(name_maps[str(qid)])]\n",
    "    else:\n",
    "        problems_s[qid]['image_feature'] = np.zeros((49, 2048))\n",
    "    if idx%10 == 0:\n",
    "        print(idx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(problems_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_example(question, context, choice, solution, answer = None, curr_le_data=None):\n",
    "    #Check if we are doing Answers or Rationale\n",
    "    if curr_le_data:\n",
    "        input = f\"Question: {question}\\nContext: {context}\\nOptions: {choice}\\n{curr_le_data}\\n\"\n",
    "        output = f\"Answer: The answer is {answer}.\"\n",
    "        text = input + f'Answer:'\n",
    "    else:\n",
    "        input = f\"Question: {question}\\nContext: {context}\\nOptions: {choice}\\n\"\n",
    "        output = f\"Solution: {solution}\"\n",
    "        text = input + f'Solution:'\n",
    "    text = text.replace(\"  \", \" \").strip()\n",
    "    output = output.replace(\"  \", \" \").strip()\n",
    "    return text, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=[\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "target_texts = []\n",
    "source_texts = []\n",
    "image_ids = []\n",
    "for k in problems_s:\n",
    "    question = problems_s[k][\"question\"]\n",
    "\n",
    "    txt_context = problems_s[k]['hint']\n",
    "    img_context = problems_s[k]['caption']\n",
    "    context = \" \".join([txt_context, img_context]).strip()\n",
    "    if context == \"\":\n",
    "        context = \"N/A\"\n",
    "\n",
    "    choices = problems_s[k]['choices']\n",
    "    choice_list = []\n",
    "    for i, c in enumerate(problems_s[k]['choices']):\n",
    "        choice_list.append(\"({}) {}\".format(options[i], c))\n",
    "    choice_txt = \" \".join(choice_list)\n",
    "\n",
    "    solution = problems_s[k][\"solution\"]\n",
    "\n",
    "    image=problems_s[k][\"image_feature\"]\n",
    "    \n",
    "    prompt,target=create_one_example(question, context, choice_txt, solution)\n",
    "    \n",
    "    target_texts.append(target)\n",
    "    source_texts.append(prompt)\n",
    "    image_ids.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"models/mm-cot-large-rationale/\")\n",
    "datacollator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, target_texts, source_texts, image_ids):\n",
    "        self.target_texts = target_texts\n",
    "        self.source_texts = source_texts\n",
    "        self.image_ids = image_ids\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image_id = self.image_ids[index]\n",
    "        target_text = str(self.target_texts[index])\n",
    "        source_text = str(self.source_texts[index])\n",
    "\n",
    "        source_text = \" \".join(source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "\n",
    "        source=tokenizer.batch_encode_plus([source_text],\n",
    "            max_length=512,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",)\n",
    "\n",
    "        target=tokenizer.batch_encode_plus([target_text],\n",
    "            max_length=512,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",)\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze().tolist()\n",
    "\n",
    "        \n",
    "        return {\"input_ids\": source_ids,\n",
    "                \"attention_mask\": source_mask,\n",
    "                \"image_ids\": torch.tensor(image_id).squeeze(),\n",
    "                \"labels\": target_ids,}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Data(target_texts,source_texts,image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics_rougel(eval_preds):\n",
    "    preds, targets = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    #Preds gets padded with -100. tokenizer can't handle negative numbers, so they get replaced with 0\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    targets = tokenizer.batch_decode(targets, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(preds, targets)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForMultimodalGeneration were not initialized from the model checkpoint at models/mm-cot-large-rationale/ and are newly initialized because the shapes did not match:\n",
      "- encoder.image_dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=T5ForMultimodalGeneration.from_pretrained(\"models/mm-cot-large-rationale/\",patch_size=(49, 2048), ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "            \"Saving\",\n",
    "            do_train=False,\n",
    "            do_eval=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            logging_strategy=\"steps\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit = 2,\n",
    "            learning_rate= 5e-5,\n",
    "            eval_accumulation_steps=None,\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=4,\n",
    "            weight_decay=0.01,\n",
    "            num_train_epochs=50,\n",
    "            predict_with_generate=True,\n",
    "            generation_max_length=512,\n",
    "            report_to=\"none\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "                        model=model,\n",
    "                        args=training_args,\n",
    "                        data_collator=datacollator,\n",
    "                        tokenizer=tokenizer,\n",
    "                        compute_metrics = compute_metrics_rougel\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_results = trainer.predict(test_dataset = data, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = predict_results.predictions, predict_results.label_ids\n",
    "preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "preds = tokenizer.batch_decode(\n",
    "            preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "targets = tokenizer.batch_decode(\n",
    "            targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "preds = [pred.strip() for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\"preds\": preds,\n",
    "                \"labels\": targets}\n",
    "#output_prediction_file = os.path.join(\"Saving\",\"predictions_ans_eval.json\")\n",
    "with open(\"Saving/predictions_ans_eval.json\", \"w\") as writer:\n",
    "        writer.write(json.dumps(output_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Solution: Will these magnets attract or repel? To find out, look at which poles are closest to each other.nThe north pole of one magnet is closest to the south pole of the other magnet. Poles that are different attract. So, these magnets will attract each other.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Solution: Will these magnets attract or repel? To find out, look at which poles are closest to each other. The north pole of one magnet is closest to the south pole of the other magnet. Poles that are different attract. So, these magnets will attract each other.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale = json.load(open(\"Saving/predictions_ans_eval.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Solution: Look at the picture of the sturgeon.nThe sturgeon's mouth is located on the underside of its head and points downward. Its mouth is adapted for bottom feeding. The sturgeon uses its mouth to find food hidden in the sediment of the ocean floor.nNow look at each animal. Figure out which animal has a similar adaptation.nThe armored catfish's mouth is located on the underside of its head. Its mouth points downward. Its mouth is adapted for bottom feeding.nThe discus's mouth is not located on the underside of its head. Its mouth is not adapted for bottom feeding.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rationale[\"7\"][\"generated_rationale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['discus', 'armored catfish']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems[\"7\"][\"choices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"models/mm-cot-large-answer/\")\n",
    "datacollator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=[\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "target_texts = []\n",
    "source_texts = []\n",
    "image_ids = []\n",
    "for k in problems_s:\n",
    "    question = problems_s[k][\"question\"]\n",
    "\n",
    "    txt_context = problems_s[k]['hint']\n",
    "    img_context = problems_s[k]['caption']\n",
    "    context = \" \".join([txt_context, img_context]).strip()\n",
    "    if context == \"\":\n",
    "        context = \"N/A\"\n",
    "\n",
    "    choices = problems_s[k]['choices']\n",
    "    choice_list = []\n",
    "    for i, c in enumerate(problems_s[k]['choices']):\n",
    "        choice_list.append(\"({}) {}\".format(options[i], c))\n",
    "    choice_txt = \" \".join(choice_list)\n",
    "\n",
    "    solution = problems_s[k][\"solution\"]\n",
    "\n",
    "    image=problems_s[k][\"image_feature\"]\n",
    "    answer = \"(\" + options[problems_s[k]['answer']] + \")\"\n",
    "    if k in rationale:\n",
    "        prompt,target=create_one_example(question, context, choice_txt, solution, answer, rationale[k][\"generated_rationale\"])\n",
    "\n",
    "        target_texts.append(target)\n",
    "        source_texts.append(prompt)\n",
    "        image_ids.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Data(target_texts,source_texts,image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ans(ans):\n",
    "    pattern = re.compile(r'The answer is \\(([A-Z])\\)')\n",
    "    res = pattern.findall(ans)\n",
    "        \n",
    "    if len(res) == 1:\n",
    "        answer = res[0]  # 'A', 'B', ...\n",
    "    else:\n",
    "        answer = \"FAILED\" \n",
    "    return answer\n",
    "\n",
    "def compute_metrics_acc(eval_preds):\n",
    "    preds, targets = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    targets = tokenizer.batch_decode(targets, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    correct = 0\n",
    "    assert len(preds) == len(targets)\n",
    "    for idx, pred in enumerate(preds):\n",
    "        reference = targets[idx]\n",
    "        reference = extract_ans(reference)\n",
    "        extract_pred = extract_ans(pred)\n",
    "        best_option = extract_pred\n",
    "        if reference == best_option:\n",
    "            correct +=1 \n",
    "    return {'accuracy': 1.0*correct/len(targets)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForMultimodalGeneration were not initialized from the model checkpoint at models/mm-cot-large-answer/ and are newly initialized because the shapes did not match:\n",
      "- encoder.image_dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=T5ForMultimodalGeneration.from_pretrained(\"models/mm-cot-large-answer/\",patch_size=(49, 2048), ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as generate, but max length is 64.\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "            \"Saving\",\n",
    "            do_train=False,\n",
    "            do_eval=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            logging_strategy=\"steps\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit = 2,\n",
    "            learning_rate= 5e-5,\n",
    "            eval_accumulation_steps=None,\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=4,\n",
    "            weight_decay=0.01,\n",
    "            num_train_epochs=50,\n",
    "            predict_with_generate=True,\n",
    "            generation_max_length=64,\n",
    "            report_to=\"none\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=datacollator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics = compute_metrics_acc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_results = trainer.predict(test_dataset = data, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = predict_results.predictions, predict_results.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "preds = tokenizer.batch_decode(\n",
    "    preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "targets = tokenizer.batch_decode(\n",
    "    targets, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_with_contion(res_pd, key, values):\n",
    "    if isinstance(values, list):\n",
    "        total_pd = res_pd[res_pd[key].isin(values)]\n",
    "    else:\n",
    "        total_pd = res_pd[res_pd[key] == values]\n",
    "    correct_pd = total_pd[total_pd['true_false'] == True]\n",
    "    if total_pd == 0:\n",
    "        acc=0\n",
    "    else:\n",
    "        acc = \"{:.2f}\".format(len(correct_pd) / len(total_pd) * 100)\n",
    "    return acc\n",
    "\n",
    "def get_scores(result_data, rationale_data, results_reference, data_file):\n",
    "    # read result file\n",
    "    results = result_data\n",
    "    num = len(results)\n",
    "    assert num == 4241\n",
    "    #print(\"number of questions:\", num)\n",
    "\n",
    "    # read data file\n",
    "    sqa_data = json.load(open(data_file))\n",
    "    # construct pandas data\n",
    "    sqa_pd = pd.DataFrame(sqa_data).T\n",
    "    res_pd = sqa_pd[sqa_pd['split'] == 'test']  # test set\n",
    "    # update data\n",
    "    for index, row in res_pd.iterrows():\n",
    "        res_pd.loc[index, 'no_context'] = True if (not row['hint'] and not row['image']) else False\n",
    "        res_pd.loc[index, 'has_text'] = True if row['hint'] else False\n",
    "        res_pd.loc[index, 'has_image'] = True if row['image'] else False\n",
    "        res_pd.loc[index, 'has_text_image'] = True if (row['hint'] and row['image']) else False\n",
    "\n",
    "        label = row['answer']\n",
    "        pred = int(results[index])\n",
    "        res_pd.loc[index, 'pred'] = pred\n",
    "        res_pd.loc[index, 'true_false'] = (label == pred)\n",
    "\n",
    "    # accuracy scores\n",
    "    acc_average = len(res_pd[res_pd['true_false'] == True]) / num * 100\n",
    "    #assert result_file.split('_')[-1] == \"{:.3f}.json\".format(acc_average)\n",
    "\n",
    "    scores = {\n",
    "            \"answer\":{\n",
    "                'acc_natural':\n",
    "                get_acc_with_contion(res_pd, 'subject', 'natural science'),\n",
    "                'acc_social':\n",
    "                get_acc_with_contion(res_pd, 'subject', 'social science'),\n",
    "                'acc_language':\n",
    "                get_acc_with_contion(res_pd, 'subject', 'language science'),\n",
    "                'acc_has_text':\n",
    "                get_acc_with_contion(res_pd, 'has_text', True),\n",
    "                'acc_has_image':\n",
    "                get_acc_with_contion(res_pd, 'has_image', True),\n",
    "                'acc_no_context':\n",
    "                get_acc_with_contion(res_pd, 'no_context', True),\n",
    "                'acc_grade_1_6':\n",
    "                get_acc_with_contion(res_pd, 'grade', ['grade1', 'grade2', 'grade3', 'grade4', 'grade5', 'grade6']),\n",
    "                'acc_grade_7_12':\n",
    "                get_acc_with_contion(res_pd, 'grade', ['grade7', 'grade8', 'grade9', 'grade10', 'grade11', 'grade12']),\n",
    "                'acc_average':\n",
    "                \"{:.2f}\".format(acc_average),\n",
    "            }}\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ans = {}\n",
    "results_rationale = {}\n",
    "results_reference = {}\n",
    "        \n",
    "num_fail = 0\n",
    "for idx, qid in enumerate([\"7\"]):\n",
    "    pred = preds[int(idx)]\n",
    "    ref = targets[int(idx)]\n",
    "    extract_pred = extract_ans(pred)\n",
    "    if extract_pred != \"FAILED\":\n",
    "        if extract_pred in options:\n",
    "            extract_pred = options.index(extract_pred)\n",
    "        else:\n",
    "            extract_pred = random.choice(range(0,len(options)))\n",
    "    else:\n",
    "        num_fail += 1\n",
    "        extract_pred = random.choice(range(len(options))) # random choose one option\n",
    "    results_ans[str(qid)] = extract_pred\n",
    "    results_rationale[str(qid)] = pred\n",
    "    results_reference[str(qid)] = ref\n",
    "\n",
    "scores = get_scores(results_ans, results_rationale, results_reference, \"data/problems.json\")\n",
    "preds = [pred.strip() for pred in preds]\n",
    "output_data = {\n",
    "        \"num_fail\": num_fail,\n",
    "        \"scores\": scores,\n",
    "        \"preds\": preds,\n",
    "        \"labels\": targets}\n",
    "\n",
    "with open(\"Saving/ans_eval.json\", \"w\") as writer:\n",
    "    writer.write(json.dumps(output_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
